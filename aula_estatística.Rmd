---
title: "Estatística no R"
author: "Maurício Bueno"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  word_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

Esta disciplina tem o objetivo de instrumentalizar o aluno para a realização das análises estatísticas mais empregadas em pesquisas em psicologia, no ambiente de programação R.
Inicialmente, vamos carregar e ativar os pacotes que iremos utilizar na disciplina.

# Carregamento dos pacotes

```{r}
# if(!require(tidyverse)) install.packages("tidyverse")
# if(!require(psych)) install.packages("psych")
# if(!require(knitr)) install.packages("knitr")
# if(!require(kableExtra)) install.packages("kableExtra")
# if(!require(expss)) install.packages("expss")
# if(!require(janitor)) install.packages("janitor")
# if(!require(pander)) install.packages('pander')
# if(!require(arsenal)) install.packages('arsenal')
# if(!require(moments)) install.packages('moments')
# if(!require(readr)) install.packages('moments')
# if(!require(readxl)) install.packages('readxl')
# if(!require(descr)) install.packages("descr")
# if(!require(rcompanion)) install.packages("rcompanion")
# if(!require(rstatix)) install.packages("rstatix")

# install.packages("Rcpp")

install.packages("GPArotation")
# Ativação dos pacotes ====

library(tidyverse)
library(psych)
library(arsenal)
# library(rstatix)
library(GPArotation)
library(corrplot)
library(janitor)
library(pander)
library(knitr)
# library(kableExtra)
# library(expss)
# library(readxl)
# library(readr)

```

# Importar banco de dados

Inicialmente vamos trabalhar com o banco de dados `dataset_mapfre.csv`, clique [aqui](https://docs.google.com/spreadsheets/d/1G0lQKeU0atMk3Q4wO5zz4HdvReRxKS68vx_Q0WbB37A/edit?usp=sharing) para baixá-lo.
Salve-o como um dataframe chamado `dataset`.

Dica: Utilize a função `read.csv`, cujo argumento `stringsAsFactors=TRUE` já salva as variáveis categóricas como fator.

Esse banco de dados se refere ao artigo "[Sintomas de depressão e ansiedade em uma amostra representativa de universitários espanhóis, portugueses e brasileiros](https://drive.google.com/file/d/1qa0iCDm2DRvh3M6a2k7ENGahceDAlPIg/view?usp=sharing)".
Sugere-se a leitura desse artigo para a compreensão dos dados.

```{r}
dataset <- read.csv("dataset_mapfre.csv",encoding="UTF-8",stringsAsFactors=TRUE)

# OBS: Para que este comando funcione, é necessário que o arquivo csv esteja no diretório de trabalho do R.

# Observar o banco de dados
glimpse(dataset)
```

# Estatística descritiva

## Tabelas de frequência com a função `table()`

```{r}
# Tabulando dados

table(dataset$country)
table1 <- table(dataset$country, dataset$sex)
table1 %>% pander()
table1 %>% kable()

# Tabulação em dataframe com porcentagens
table(dataset$country) %>% 
  as.data.frame() %>% 
  mutate("%" = Freq*100/sum(Freq)) %>% 
  pander()

table(dataset$country, dataset$sex) %>% 
  as.data.frame() %>% 
  mutate("%" = Freq*100/sum(Freq)) %>% 
  pander()

table(dataset$country, dataset$sex, dataset$bai_class) %>% 
  as.data.frame() %>% 
  mutate("%" = Freq*100/sum(Freq)) %>% 
  pander()
```

## Tabelas de frequência com a função `count()`

```{r}
# Contar uma variável
dataset %>% 
  count(sex) %>% 
  mutate("%" = n*100/sum(n)) %>%
  pander()

# Contar uma variável x em função de uma y, com totalização integral.

dataset %>% 
  count(sex,country) %>% 
  mutate("%" = n*100/sum(n)) %>%
  pander()

# Contar uma variável x em função de outra y, com totalização pelas categorias de x. 

dataset %>% 
  filter(!is.na(sex)) %>% 
  group_by(sex) %>% 
  count(country) %>% 
  mutate(porc = n/sum(n)*100) %>% 
  pander()
```

## Tabelas de frequência com o pacote `janitor`

```{r}
# install.packages("janitor")
# library(janitor)

# usando a função tabyl() do pacote janitor
# contar quantas pessoas de cada país.

dataset %>%             # pegar o dataframe dataset
  tabyl(country) %>%    # use a função tabyl para tabular as categorias de país
  pander()              # utilize o pander para fazer a tabela

dataset %>%             # pegar o dataframe dataset
  tabyl(country) %>%    # use a função tabyl para tabular as categorias de país
  adorn_totals() %>%    # acrescente uma linha com os totais
  pander()              # utilize o pander para fazer a tabela

dataset %>%             # pegar o dataframe dataset
  tabyl(country) %>%    # use a função tabyl para tabular as categorias de país
  adorn_totals() %>%    # acrescente uma linha com os totais
  adorn_pct_formatting(digits = 1) %>% 
  pander()              # utilize o pander para fazer a tabela

# contar por gênero e país (retirando NAs)
dataset %>% 
  filter(!is.na(sex)) %>% 
  tabyl(sex,country) %>% 
  adorn_totals() %>% 
  pander()

dataset %>% 
  filter(!is.na(sex)) %>% 
  tabyl(sex,country) %>% 
  adorn_totals(c("row","col")) %>% 
  pander()

dataset %>% 
  drop_na(sex) %>%        # outra forma de eliminar os NAs
  tabyl(sex,country) %>% 
  adorn_totals(c("row","col")) %>% 
  pander()

dataset %>% 
  tabyl(sex,country,show_na = FALSE) %>% # eliminar NAs com argumento do próprio janitor
  adorn_totals(c("row","col")) %>% 
  pander()

# contar por gênero e país, acrescentando funções
dataset %>% 
  tabyl(country,sex, show_na = FALSE) %>%           # função de tabulação do janitor
  adorn_totals(c("row","col")) %>% # adiciona o N
  adorn_percentages("all") %>%     # adiciona porcentagens (One of "row", "col", or "all".)
  adorn_pct_formatting(
    rounding = "half up",          # arredondar do cinco pra cima
    digits = 0) %>%                # número de casas decimais
  adorn_ns() %>%                   # mostra N e % juntas
  pander()                         # melhora a visualização dos dados.

# adição de um qui-quadrado para a distribuição
# para rodar essa análise tem que tirar os totais.

dataset %>%
  filter(!is.na(sex)) %>%
  tabyl(country,sex) %>%           # função de tabulação do janitor
  adorn_totals(c("row","col")) %>% # adiciona o N
  chisq.test()
  # pander()                         # melhora a visualização dos dados.
```

## Gráficos de frequência com o pacote `ggplot2`

### Gráficos de barras

```{r}

# Gráfico mais simples possível (contagem simples)
ggplot(dataset, aes(x = country)) + 
  geom_bar(color = "black", fill = "dodgerblue")

# Gráfico de barras mais complexo (4)

## Camada 1 - gráfico de barras com algumas melhorias visuais

ggplot(data = dataset,             # base de dados
       mapping = aes(x = country,  # informação que vai no eixo x
                     y = ..prop.., # informação que vai no eixo y: forma de indicar que é a proporção
                     group = 1)) + 
  geom_bar(stat = "count",         # definir que a estatística parte da contagem
           fill = "dodgerblue4",   # cor das barras
           width = .5,             # espessura das barras
           color = "black") +      # cor da borda das barras

## Camada 2 - colocar eixo y em porcentagem
    scale_y_continuous(labels = scales::percent_format()) +

## Camada 3 - colocar o valor das porcentagens nas barras
  geom_text(aes(label = scales::percent(..prop..),
                y = ..prop..),
            stat = "count", 
            color = "white",
            size = 5, 
            position = position_stack(vjust = 0.5)) +
  
## Camada 4 - Titulos, labels
    labs(x = "País",
       y = "Porcentagem",
       title = "Número de participantes por país",
       subtitle = "Brasil, Portugal e Espanha",
       caption = "Núcleo de Estudos em Avaliação Psicológica - NEAP")

# Gráfico de barras empilhadas (utilizando o geom_col)
# OBS.: O geom_col é parecido com o geom_bar, mas o primeiro só aceita dados diretos.
# Por isso, nesse caso, é preciso preparar o dataframe antes.

## Preparação do data frame
dataset %>% 
  count(country) %>% 
  mutate(pct = n/sum(n)) %>% 
## Camada 1
  ggplot(.,aes(x = "", 
               y = pct, 
               fill = country)) + # se o argumento fill for colocado dentro de aes, sai uma cor por barra 
  geom_col(color = "black") +
## Camada 2
  geom_text(aes(label = scales::percent(round(pct,3))),
            position = position_stack(vjust = 0.5)) + 
## Camada 3
  labs(title = "Proporção de participantes por país")
```

### Gráficos de pizza

```{r}
# é um gráfico de barras empilhadas transformado em polar

## preparação dos dados
dataset %>% 
  count(country) %>% 
  mutate(pct = n/sum(n)) %>%
# Camada 1
  ggplot(.,aes(x = "", y = pct, fill = country)) + 
  geom_col(color = "black") + 
# Camada 2
  geom_text(aes(label = scales::percent(round(pct,3))),
            position = position_stack(vjust = 0.5)) + 
# Camada 3
  coord_polar(theta = "y") +
# Camada 4 
  labs(title = "Proporção de participantes em cada país") + 
   xlab(NULL) + 
   ylab(NULL)

```

### Gráficos de linhas

```{r}
# Contar categorias dentro de uma variável (nível de depressão)

dataset %>%
  drop_na(bdi_class) %>% 
  ggplot() +
  geom_line(aes(x = bdi_class, group = 1), stat = "count", size = 1.2) +
  labs(title = "Classificações de Depressão - Frequências",
       x = "Classificação",
       y = "Frequências")

# Misturando variáveis (frequência de classificações por gênero)

## preparação dos dados
dataset %>%
  drop_na(bdi_class,sex) %>% 
  group_by(sex) %>% 
  count(bdi_class,sex) %>%
  mutate(porc = n*100/(sum(n))) %>% 
## camada 1
  ggplot() +
  geom_line(aes(x = bdi_class, 
                y = porc, 
                group = sex, 
                color = sex), 
            size = 1.0) +
## camada 2
  labs(title = "Classificações de Depressão por gênero",
       x = "Classificação",
       y = "Frequências")
```

# Estatísticas

Este curso está didaticamente dividido em três tipos de análises estatísticas:

1.  Estatísticas descritivas, que incluem as medidas de tendência central e de dispersão.
2.  Estatísticas que ajudam a compreender associação entre variáveis: qui-quadrado, correlação, regressão e análise fatorial.
3.  Efeitos de grupo em outras variáveis

As próximas seções apresentam recursos para a realização dessas análises.

# Estatísticas descritivas

A descrição dos dados geralmente é realizada pelas seguintes estatísticas: Média, desvio padrão, mediana, amplitude, mínimo, máximo, assimetria e curtose.
Algumas dessas análises (média, desvio padrão e mediana) informam sobre o ponto central da distribuição das medidas.
Outras, como o desvio padrão, a amplitude, a assimetria e a curtose informam sobre a forma como os valores se afastam do ponto central.

Aqui, é importante atentar para a **distribuição normal**, cujas características são apresentadas na Figura abaixo.

```{r echo=FALSE}
distnorm <- rnorm(100000, mean = 10, sd = 2)
hist(distnorm,
     col = "lightblue",
     main = "Distribuição Normal",
     freq = F,
     xlab = "Escores",
     ylab = "Frequência",
     breaks = 21)
curve(dnorm(x, mean = mean(distnorm), sd = sd(distnorm)), add = TRUE)

# distnorm1 <- data.frame(escores=rnorm(100000,10,2))
# 
# ggplot(distnorm1, aes(x = escores)) + 
#   geom_histogram(aes(y = ..density..),
#                  fill = "pink",
#                  color = "black",
#                  alpha = 1,
#                  bins = 25) +
#   stat_function(fun = dnorm, args = list(mean = mean(distnorm1$escores), sd = sd(distnorm1$escores))) + 
#   labs(title = "Distribuição Normal",
#        x = "Escores",
#        y = "Densidade")
```

### Medidas de tendência central

```{r}
## mean()
mean(dataset$age, na.rm = TRUE)

## sd()
sd(dataset$age, na.rm = TRUE)

## median()
median(dataset$age, na.rm = TRUE)
```

### Medidas de dispersão

```{r}
## min()
min(dataset$age, na.rm = TRUE)

## max()
max(dataset$age, na.rm = TRUE)

## range()
range(dataset$age, na.rm = TRUE)

## skewness()
# moments::skewness(dataset$age, na.rm = TRUE) # pacote moments 
skew(dataset$age, na.rm = TRUE)              # pacote psych

## kurtosis()
# moments::kurtosis(dataset$age, na.rm = TRUE) # pacote moments
kurtosi(dataset$age, na.rm = TRUE)           # pacote psych

```

## Funções que sumariam dados

```{r}
# summary() {base}
dataset %>% select(age) %>% summary()

# summarise() {tidyverse}
dataset %>% 
  drop_na(sex,age,bai_sum,bdi_sum) %>%
  group_by(sex) %>% 
  summarise(idade = mean(age),
            BAI = mean(bai_sum),
            BDI = mean(bdi_sum))

# describe() {psych}
describe(dataset$age, na.rm = TRUE) %>% pander()
describe(dataset$age, na.rm = TRUE) %>% pander(format = "markdown")

# tableby() {arsenal}
tableby(country ~ bdi_sum + bai_sum, # calcule as descritivas de bdi e bai por country 
        test = FALSE,                # se FALSE,  não faz teste de significância 
        data = dataset) %>%          # especificar base de dados
  summary(text=TRUE)                 # tem que colocar text=TRUE senão aparecem uns códigos estranhos porque o R entende o espaço como código. Assim a tabela fica bacaninha.

## mesma função mas para a variável sexo.
tableby(sex ~ bdi_sum + bai_sum,
        test = FALSE,
        data = dataset) %>% 
  summary(text=TRUE)

## Pode descrever apenas uma variável em função de sexo
tableby(sex ~ age, data = dataset) %>% summary(text = TRUE)

## Pode descrever somenta a variável contínua sem ser em função de alguma outra variável.
tableby( ~ age + bdi_sum + bai_sum, data = dataset) %>% summary(text = TRUE)

## Também pode dar as médias em função do sexo, estratificado por país
tableby(sex ~ age + bdi_sum + bai_sum, data = dataset, strata = country) %>% summary(text = TRUE)

## ou o contrário
tableby(country ~ age + bdi_sum + bai_sum, data = dataset, strata = sex) %>% summary(text = TRUE)

## Interação entre variáveis nominais

tableby(interaction(sex, country) ~ bdi_sum + bai_sum,
        test = FALSE,
        data = dataset) %>% 
  summary(text = TRUE)
```

## Gráficos de medidas de tendência central e de dispersão

### Gráficos de barras

```{r}
# Gráfico de barras - Gráfico básico

ggplot() +
    # Camada 1 - Barras
    geom_bar(data = dataset,
           mapping = aes(x = country,
                         y = bai_sum),
           stat = "summary",
           fun = mean,
           na.rm = TRUE)

# Gráfico de barras básico, com embelezamento

ggplot() +
    # Camada 1 - Barras com embelezamento
    geom_bar(data = dataset,
           mapping = aes(x = country,
                         y = bai_sum),
           stat = "summary",
           fun = mean,
           na.rm = TRUE,
           fill = "dodgerblue",
           color = "black")


# Gráfico de barras completo: Básico + Outros geoms (barra de erros e texto)

médias_BAI <- # dataframe com as informações que eu vou inserir na camada 2 do gráfico
dataset %>% group_by(country) %>% summarise(mean = mean(bai_sum, na.rm = TRUE))

ggplot() +
  # Camada 1 - Barras com embelezamento
  geom_bar(data = dataset,
           mapping = aes(x = country,
                         y = bai_sum),
           stat = "summary",
           fun = mean,
           na.rm = TRUE,
           fill = "dodgerblue",
           color = "black") +
  # Camada 2 - Barra de Erro
  stat_summary(data = dataset, 
               mapping = aes(x=country, y=bai_sum),
               geom = "errorbar",
               fun.data = mean_se,
               width = 0.5) +
  # Camada 3 - Texto (valor das médias dentro das barras)
  geom_text(data = médias_BAI,
            mapping = aes(x = country,
                          y = mean,
                          label = format(mean,digits = 1,nsmall = 1)),
            fontface = "bold", size = 5, vjust = 3)

# Gráfico de barras (step3): Básico + Texto + Complementos (lables, themes e coord)

ggplot() +
  # Camada 1 - Barras com embelezamento
  geom_bar(data = dataset,
           mapping = aes(x = country,
                         y = bai_sum),
           stat = "summary",
           fun = mean,
           na.rm = TRUE,
           fill = "dodgerblue",
           color = "black") +
  # Camada 2 - Barra de Erro
  stat_summary(data = dataset, mapping = aes(x=country, y=bai_sum),
               geom = "errorbar",fun.data = mean_se,width = 0.5) +
  # Camada 3 - Texto (valor das médias dentro das barras)
  geom_text(data = médias_BAI,
            mapping = aes(x = country,
                          y = mean,
                          label = format(mean,digits = 1,nsmall = 1)),
            fontface = "bold", size = 5, vjust = 3) +
  # Camada 4 - Labels (título, subtítulo, eixos, caption, theme, coord)
  coord_cartesian(ylim = c(0,10)) +   # mudar escala do eixo y
  # coord_flip() + (se ligar, é necessário ajustar vjust e hjust)
  labs(title = "Média de Ansiedade por país",
       subtitle = "Ansidade medida pelo BAI",
       x = "País",
       y = "Média de Ansiedade (BAI)",
       caption = "Núcleo de Estudos em Avaliação Psicológica") + 
  theme_gray()

# colorindo por país
# tem que colocar o fill dentro do aes
ggplot() +
  # Camada 1 - Barras com embelezamento
  geom_bar(data = dataset,
           mapping = aes(x = country,
                         y = bai_sum,
           fill = country),
           stat = "summary",
           fun = mean,
           na.rm = TRUE,
           color = "black") +
  # Camada 2 - Barras de erro
  stat_summary(data = dataset, 
               mapping = aes(x=country, y=bai_sum),
               geom = "errorbar",
               fun.data = mean_se,width = 0.5,
               na.rm = TRUE) +
  # Camada 3 - Texto (valor das médias dentro das barras)
  geom_text(data = médias_BAI,
            mapping = aes(x = country,
                          y = mean,
                          label = format(mean,digits = 1,nsmall = 1)),
            fontface = "bold", size = 5, vjust = 3) + # mudar vjust e hjust
  # Camada 4 - Labels (título, subtítulo, eixos, caption, theme, coord)
  coord_cartesian(ylim = c(0,10)) +   # mudar escala do eixo y
  # coord_flip() +                      # gráfico horizontal
  labs(title = "Média de Ansiedade por país",
       subtitle = "Ansidade medida pelo BAI",
       x = "País",
       y = "Média de Ansiedade (BAI)",
       caption = "Núcleo de Estudos em Avaliação Psicológica") + 
  theme_gray()
```

### Boxplot

```{r}
# Gráfico básico
ggplot(data = dataset, 
       mapping = aes(x = country, y = bai_sum)) +
  geom_boxplot(fill = "violet") +
  labs(title = "Boxplots BAI por país",
       x = "Países",
       y = "Ansiedade_BAI")

# Gráfico mais completo 

## Passo 1 - Criação de um dataframe com informações sobre a mediana
median_boxplot <- 
  dataset %>% 
  group_by(country) %>% 
  summarise(mediana = median(bai_sum, na.rm = TRUE))

## Gráfico
### camada 1
ggplot() +
  geom_boxplot(data = dataset, 
               mapping = aes(x = country, y = bai_sum, fill = country),
               show.legend = FALSE, 
               na.rm = TRUE) +
### camada 2
  geom_text(data = median_boxplot,
            aes(x = country, 
                y = mediana,
                label = format(mediana, digits = 1, nsmall = 1)),
            fontface = "bold", size = 4, vjust = -.5) +
### camada 3  
  labs(title = "Boxplots BAI por país",
       x = "Países",
       y = "Ansiedade_BAI") +
### camada 4
  theme_bw()
```

# Gráfico de pontos

```{r}

# Gráfico simples
ggplot(dataset, aes(x = country, y = bai_sum)) +
  geom_point(stat = "summary",fun = "mean",na.rm = TRUE)

## Outra forma de construir o "gráfico simples"
ggplot(dataset, aes(x = country, y = bai_sum)) +
  stat_summary(geom = "point", fun = "mean")

# "Gráfico simples" + Barra de erro

ggplot(dataset,aes(x = country, y = bai_sum)) +
  geom_point(stat = "summary", fun = "mean", size = 3) +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3)

# "Gráfico simples" + Barra de erro + Ajuste das dimensões
ggplot(dataset,aes(x = country, y = bai_sum)) +
  geom_point(stat = "summary", fun = "mean", na.rm = TRUE) +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.3) + 
  coord_cartesian(ylim = c(0,12))

## install.packages("pacman")
pacman::p_load(ggpubr) # ggpubr é um pacote que adiciona algumas funções ao ggplot
                       # com isso fica possível usar a função mean_ci
                       # para calcular intervalo de confiança.

# OBS: a função p_load do pacote pacman verifica se um pacote está instalado e instala se for necessário.

### Barrinha de intervalo de confiança
ggplot(dataset,aes(x = country, y = bai_sum)) +
  geom_point(stat = "summary", fun = "mean", na.rm = TRUE) +
  geom_errorbar(stat = "summary", fun.data = "mean_ci", width = 0.3, na.rm = TRUE) + 
  coord_cartesian(ylim = c(5,12))

### Barrinha de desvio padrão
ggplot(dataset,aes(x = country, y = bai_sum)) +
  geom_point(stat = "summary", fun = "mean", na.rm = TRUE) +
  geom_errorbar(stat = "summary", fun.data = "mean_sd", width = 0.3, na.rm = TRUE) + 
  coord_cartesian(ylim = c(0,20))
```

------------------------------------------------------------------------

# Análises de associação entre variáveis

Muitas vezes, o pesquisador está interessado no estudo ou análise de relações entre variáveis.
Nesses casos, ele recorrerá a estatísticas como o teste de qui-quadrado (caso tenha variáveis do tipo nominal ou categórica), coeficiente de correlação de Pearson ou regressão linear, entre outras, quando as variáveis forem contínuas.

## Testes de Qui-Quadrado

O teste de qui-quadrado é uma medida de associação entre variáveis categóricas, das quais só temos a frequência de ocorrência.
Existem três tipos de testes de qui-quadrado:

-   de aderência: quando se deseja verificar as distribuições de probabilidades de cada categoria de uma variável em relação a um valor teórico esperado.

-   de homogeneidade: quando se deseja verificar se as distribuições das categorias são as mesmas para diferentes subpopulações de interesse.

-   de independência: verficiar se duas variáveis categóricas são independentes

Neste caso, vamos fazer uma análise de qui-quadrado de independência, a mais comumumente utilizada, para verificar se há uma associação entre país e sexo dos participantes do banco de dados dataset.
O primeiro passo para isso é montar uma tabela de contingência.

### Tabela de Contingência

Nesse caso, vamos usar a função da base `table()` para gerar uma tabela 2 x 3, duas linhas e três colunas, em que as linhas representarão os sexos masculino e feminino, e as colunas representarão os países: Brasil, Portugal e Espanha.

```{r}
tabcont_sex_country <- table(dataset$sex,     # linhas
                             dataset$country) # colunas
```

O próximo passo é realizar a análise de qui-quadrado em si.

### Análise de qui-quadrado.

```{r}
# É bom salvá-la em um objeto porque tem mais informações do que as que são mostradas na análise.

options(scipen = 999) # função para tirar a notação científica de potência.

# Qui-quadrado
chi_sex_country <- chisq.test(tabcont_sex_country)
```

Nesse caso, o X^2^ foi de 48,458, para 2 graus de liberdade, e o valor de p foi igual a 3.003e-11 (lê-se 3.003 x 10^-11^).
Ou seja, o qui-quadrado foi estatisticamente significativo, indicando que os participantes dos sexos masculino e feminino **não foram** distribuídos homogeneamente dentro dos países.Isso poderia ser relatado da seguinte forma:

*Foi realizada uma análise de qui-quadrado para verificar se havia homogeneidade na distribuição dos sexos masculino e feminino entre os participantes de cada país (Brasil, Portugla e Espanha). Observou-se que a distribuição não foi homogênea (X^2^ = 48,458; gl = 2, p-valor = 3,003e-11) e que a falta de homogeneidade se deve aos participantes espanhois, cuja porcentagem de mulheres e mais elevada do que a de homens. Portugal e Brasil apresentaram porcentagens bem parecidas quanto a distribuição por gênero.*

### Representação em tabela

```{r}
dataset %>% 
  tabyl(sex,country,show_na = FALSE) %>%     # função de tabulação do janitor
  adorn_totals(c("row","col")) %>%           # adiciona o N
  adorn_percentages("col") %>%               # adiciona porcentagens (One of "row", "col", or "all".)
  adorn_pct_formatting(                
    rounding = "half up",                    # arredondar do cinco pra cima
    digits = 0) %>%                          # número de casas decimais
  adorn_ns() %>%                             # mostra N e % juntas
  pander()                                   # melhora a visualização dos dados.
```

Pode-se observar que as distribuições de Brasil e Portugal são muito próximas, mas a da Espanha é bastante diferente, o que deve estar produzindo a significância estatística.

### Representação gráfica

```{r}
ggplot(dataset, aes(x = country, fill = sex)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proporção Sexo x País",
       x = "País", 
       y = "Proporção", 
       fill = "Sex")
```

## Coeficientes de correlação: Pearson, Spearman e Kendall

Quando o interesse do pesquisador é investigar o grau de associação entre variáveis, ele pode empregar o Coeficiente de Correlação de Pearson.
Os resultados dessa análise podem variar de -1 a 1.
Resultados positivos revelam associação diretamente proporcional entre as variáveis, resultados negativos revelam associação inversamente proporcional entre as variáveis e resultados próximos de zero indicam que não há associação entre as variáveis.

Quando os dados apresentam distribuição normal, usamos o coeficiente de correlação de Pearson.
Para dados que não apresentam essa distribuição, usamos a correlação de Kendal ou Spearman.
Para isso, vamos usar a função `corrplot()` do pacote `corrplot`.
Portanto, é necessário instalar ( `install.packages("corrplot")`) e ativar (`library(corrplot)`) esse pacote.

### Matriz de correlações

```{r}
# matriz completa (correlações entre todas as variáveis)
matriz_comp <- corr.test(dataset[,c(3,89,92)])

## Visualização dos resultados
matriz_comp

## Visualização somente das correlações
matriz_comp$r

# matriz parcial (correlações de ansiedade/depressão com idade)
matriz_parc <- corr.test(dataset[,c(89,92)],dataset[,3], method = "pearson")

## Visualização dos resultados
matriz_parc

## Visualização somente das correlações
matriz_parc$r
```

Os resultados são apresentados em três matrizes.
A primeira com os coeficientes de correlação, a segunda com o número de participantes empregado para o cálculo dos coeficientes, e a terceira com os p-values.
Para um coeficiente de correlação ser estatisticamente significativo, o p-value deve ser menor que 0,05, pois isso indica que o valor do coeficiente de correlação obtido seria muito raro em um banco de dados aleatórios.

Na matriz completa, nota-se que as correlações de bdi_sum e bai_sum com idade foram próximas de zero e com p-vlaues de 0,43 e 0,06, respectivamente, indicando que não há associação entre de ansiedade e depressão com idade.
Mas a correlação entre bdi_sum e bai_sum foi de 0,60 com p-value igual a 0,00 (nesses casos, relata-se p \< 0,01).
Exemplo de relato:

*Observou-se que houve correlação positiva e estatisticamente significativa entre ansiedade e depressão (r = 0,60; p \< 0,01) e correlações negativas e não significativas de ansiedade (r = -0,04; p = 0,06) e depressão (r = -0.02, p = 0,43) com idade.*

### Representação gráfica das correlações

```{r}
# gráfico de dispersão de pontos

ggplot(dataset, aes(x=bdi_sum,y=bai_sum)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  labs(title ="Correlação entre Depressão e Ansiedade", 
       x = "Depressão", 
       y = "Ansiedade") +
  theme_classic()

# adicionar informações sobre a curva de regressão

# summary(lm_ans_dep) # esse linear model foi rodado anteriormente


ggplot(dataset, aes(x=bdi_sum,y=bai_sum)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  labs(title ="Correlação entre Depressão e Ansiedade", 
       x = "Depressão", 
       y = "Ansiedade") +
  theme_classic() + 
  annotate("text",x=20,y=65,label="Adjusted R-squared:  0.3596") + 
  annotate("text",x=20,y=60,label="bai_sum = 4.36700 + 0.57206*bdi_sum")
  

# matriz de correlações usando o corrplot
corrplot::corrplot(matriz_comp$r, 
                   method = "shade",
                   type = "lower",
                   order = "hclust",
                   addCoef.col = TRUE)

# opções de método: "circle", "square", "ellipse", "number", "shade", "color", "pie
# opções de tipo: "lower", "upper", "full"
# opção de ordem: "hclust"- organiza as correlações hierarquicamente
#                 "original" - ordem das variáveis no banco
#                 "AOE" - angular order of eigenvectors
#                 "alphabet" - ordem alfabética
```

## Regressão Linear Múltipla

A análise de regressão linear avança em relação ao coeficiente de correlação, por estabelecer QUANTO uma variável varia em função de outra.
Por exemplo, é possível predizer ansiedade a partir da depressão?
Quanto a ansiedade varia em função da depressão?

Então, nesse caso, teremos a ansiedade como variável dependente (aquela que se quer prever) e a depressão como independente (aquela que será a preditora).
Para verificar isso, usaremos a função `lm()` do pacote stats da base do R.
LM são as iniciais de linear model.

```{r}
mod_ans_dep <- lm(data = dataset,       # onde estão os dados
                  bai_sum ~ bdi_sum,    # depressão predizendo ansiedade
                  na.action = na.omit)  # ignorar NAs

# OBS.: Estamos salvando os resultados do modelo no objeto mod_ans_dep. Para ver o resultado, podemos usar a função summary()

summary(mod_ans_dep)
```

Há três tipos de resultados importantes na saída dessta análise:

1.  F-statistic: 1094 on 1 and 1946 DF, p-value: \< 2.2e-16

Essa é a última linha dos resultados, mas a primeira a ser observada.
A estatística F compara se a associação entre as duas variáveis consideradas (ansiedade e depressão) é significativamente diferente da associação entre duas variáveis aleatórias (cuja correlação costuma ser muito próxima de zero).
Portanto, se a estatística F for estatisticamente significativa (p-value \< 0,05), isso indica que pode ser possível descrever a variação de uma variável (ansiedade) em função da outra (depressão), porque provavelmente há associação entre ansiedade e depressão.

2.  Multiple R-squared: 0.3599, Adjusted R-squared: 0.3596

Indica a % de variância de ansiedade (variação entre as pessoas) que é explicada pela variância de depressão.
Há dois índices, o R-quadrado e o R-quadrado ajustado.
O segundo é ajustado para o número de variáveis preditoras que há no modelo e é mais adequado que o R-quadrado.

Nesse caso, utilizando o R-quadrado ajustado, o resultado indica que 35,96% da variância de ansiedade é explicada pela variância de depressão.

3.  Os coeficientes de regressão

Os coeficientes são os valores que entram na equação: $y_i = \alpha + \beta x_i + e_i$

Nesse caso, o $\alpha = 2,69295$ (intercepto) e $\beta = 0,62919$ .
Isso indica que, para predizer a pontuação em ansiedade de sujeito i, basta pegar sua pontuação em depressão, multiplicar por 0,62919 e somar 2,69295 e o erro de estimação desse sujeito (diferença entre o predito e o real).
Nota-se que o p-valor de bdi_sum foi estatísticamente significativo, com p = 2 .
10^-16^ , indicando que a cada ponto que depressão sobre, a ansiedade sobre 0,62919 pontos.

Exemplo de relato dos resultados: *Foi realizada uma análise de regressão para verificar a possibilidade de predizer ansiedade a partir da depressão. Observou-se que essa predição é viável (F=1094, DF = 1, p-value \< 2.2e-16), que ela explica 36% da variância total de ansiedade e que a cada aumento de um ponto em depressão, a pontuação em ansiedade tende a aumentar 0,63 pontos.*

A análise de regressão linear múltipla, é muito semelhante à simples.
Apenas envolvem mais de uma variável independente.
Vamos ver esse exemplo, com o banco de dados big_five, do qual vamos selecionar apenas os dados dos participantes brasileiros.
Para isso, no seu computador, copie o banco de dados `big_five` para o diretório em que está realizando estas análises.

```{r}
# importar o "big_five.rds" para dentro do R.
big_five <- readRDS("big_five.rds")

# selecionar somente os participantes brasileiros do df big_five.
big_five_BR <- big_five %>% filter(país == "BR")
```

Em seguida, gerar o modelo e sumariar os dados.

```{r}
# rodando a regressão (usando a função lm() do pacote stats (básico do R)
# lm vem de linear model
mod <- lm(idade ~ extr+neur+amab+cons+aber, # big-five predizendo idade
          data=big_five_BR,                 # onde estão os dados
          na.action = na.omit)              # ignorar NAs

## Interpretação do modelo
summary(mod)

# estatística F: comparação do modelo real com o modelo nulo (sem variáveis independentes). Então, só faz sentido usar o modelo real se ele for melhor que o modelo nulo. Para isso, o valor de p < 0,05.
# R-squared: porcentagem da variância que é explicada pelo modelo. O R-squared corrige pelo número de variáveis, o que permite comparar modelos com diferentes números de variáveis independentes. 
```

Reportando resultados: O modelo experimental é significativamente diferente de um modelo nulo (F = 6,768, gl = 169, p = 8,898 . 10^-6^), e foi capaz de explicar 14,2% da variância da idade por meio dos traços de extroversão e conscienciosidade.

```{r}
mod$coefficients %>% as.data.frame() %>% pander()
```

## Análise fatorial exploratória

A análise fatorial exploratório é um recurso estatístico complexo, que usa, na sua base, coeficientes de correlação (ou de covariância) e regressões, com o objetivo de identificar conjuntos de variáveis (itens) que se comportam de forma semelhante ao longo dos sujeitos.
Essas variáveis são, geralmente, itens de um teste.
Então, comportar-se de forma semelhante ao longo dos sujeitos quer dizer que esse grupo de variáveis (itens) apresenta magnitudes semelhantes no mesmo sujeito.
Por exemplo, um grupo de itens que é respondido com valores altos pelo sujeito s1, baixos pelo sujeito s2, médios pelo sujeito s3, e assim por diante....
não importa quem responda, as respostas são sempre coerentes entre si (baixas, médias ou altas).

Por essas características, é muito utilizada na construção de instrumentos de avaliação psicológica como um recurso para a investigação da **validade com base na estrutura interna**.

A realização de uma análise fatorial exploratória sguem alguns passos:

1.  Verificação das condições para a realização da análise fatorial
2.  Identificação do número de fatores a serem retidos na análise
3.  Extração dos fatores

Então, vamos ver uma fase por vez, usando o dataframe `big_five`.

#### Verificação das condições para a realização da análise fatorial

Geralmente, são empregados dois indicadores para essa finalidade: o índice de Kaizer-Meyer-Olkin (KMO) e o Teste de Esfericidade de Bartlett.

O KMO indica se há correlações suficientes para a realização da análise fatorial.
Pode variar de zero a 1 e **deve ser superior a 0,5** para que se possa prosseguir com a análise de forma adequada.

```{r}
names(big_five)
big_five %>% select(9:58) %>% KMO()
```

O KMO total foi de 0,91, indicando haver condições suficientemente boas para o emprego da análise fatorial.

O Teste de esfericidade de Bartlett indica se a matriz de correlações é significativamente diferente de uma matriz identidade (uma matriz em que todos os valores da diagnonal são iguais a 1 e os demais são iguas a zero).
Ou seja, nesse caso, o qui-quadrado entre a matriz de dados experimentais e uma matriz de dados aleatórios.
Se p-valor \< 0,05, então a matriz é significativamente diferente de uma matriz identidade.

```{r}
big_five %>% select(9:58) %>% bartlett.test()
```

Nesse caso, o p-valor \< 2.2e-16 indica que a matriz de correlações é significativamente diferente de uma matriz identidade.

Portanto, neste caso, tanto o MO quanto o Teste de Esfericidade de Bartlett indicaram haver condições para a realização da análise fatorial.
Por isso, passou-se à segunda etapa.

#### Identificação do número de fatores a serem retidos na análise

Para essa verificação, três métodos são comumente utilizados:

1.  A regra do eigenvalue maior que 1
2.  O scree-plot (ou, gráfico de sedimentação)
3.  A análise paralela

O scree-plot ajuda a resolver os dois primeiros casos.

```{r}
big_five %>% select(9:58) %>% 
  scree(pc = FALSE) # extrair fatores, não extrair componentes
```

Pela inspeção do scree-plot, observa-se a curva formada pelos pontos e seleciona-se os que apresentam eigenvalues mais elevados, após o ponto de inflexão da curva.
Por esse critério, poder-se ia selecionar 6 pontos (fatores)(regra 2).
No entanto, há apenas 5 deles com eigenvalues superiores a 1 (regra 1).
Portanto, selecionam-se cinco fatores, que coincide com o número de fatores esperado pela teoria (big five).

A análise paralela é um procedimento que compara eigenvalues obtidos com os dados experimentais e com matrizes de dados aleatórios.
Seleciona-se o número de fatores que apresentaram eigenvalues experimentias superiores aos aleatórios.

```{r}
parallel_bigfive <- 
big_five %>% select(9:58) %>% fa.parallel()
```

A análise paralela resultou na indicação de 10 fatores.
Veja tabela abaixo.

```{r}
data.frame(as.data.frame(parallel_bigfive$fa.values),
           as.data.frame(parallel_bigfive$fa.sim)) %>% pander()
```

Note que os 10 primeiros valores experimentais (col_1) são superiores aos respectivos valores simulados (col_2).
No entanto, somente os cinco primeiros valores experimentais são superiores a 1.
Portanto, depreende-se do conjunto dos três critérios que **devem ser retidos cinco fatores**.

#### Extração dos fatores

Uma curiosidade... em uma análise fatorial, é esperado que os itens de um mesmo fator se correlacionem mais entre si do que com itens de outros fatores.
Isso pode ser observado na plotagem das correlações entre os itens.

```{r}
# matriz de correlações usando o corrplot

corrplot::corrplot(corr.test(big_five[,9:58])$r, 
                   order = "original",
                   type = "lower",
                   title = "Correlação entre os itens - big_five")
```

Essa correlação entre os itens não precisa ser realizada antes da análise fatorial.
Fizemos aqui apenas a título de curiosidade.
Em seguida, passamos à extração dos fatores propriamente dita.

```{r}
fa_big_five <- big_five %>% select(9:58) %>% fa(nfactors = 5, 
                                                rotate = "geominQ", 
                                                fm = "uls")

# para visualizar as cargas fatoriais
summary(fa_big_five)
```

## Fidedignidade

Após a identificação da (validade com base na) estrutura interna do instrumento, em psicometria (construção de instrumentos), costuma-se verificar a fidedignidade (precisão) dos fatores.
Os principais métodos reportados são o coeficiente alfa de Cronbach e o ômega de McDonald.

Ambos podem ser encontrados na função `omega()` do pacote `psych`.

```{r}
# os itens negativos já foram invertidos anteriormente.

rely_E <- big_five %>% select( 9:18) %>% omega(poly=TRUE, digits=2, plot=FALSE)
rely_N <- big_five %>% select(19:28) %>% omega(poly=TRUE, digits=2, plot=FALSE)
rely_A <- big_five %>% select(29:38) %>% omega(poly=TRUE, digits=2, plot=FALSE)
rely_C <- big_five %>% select(39:48) %>% omega(poly=TRUE, digits=2, plot=FALSE)
rely_O <- big_five %>% select(49:58) %>% omega(poly=TRUE, digits=2, plot=FALSE)

# apresentação numa tabela

data.frame(Fatores = c('Extroversão', 
                       'Neuroticismo', 
                       'Amabilidade', 
                       'Conscienciosidade', 
                       'Abertura'),
           Alfa = c(rely_E$alpha, 
                    rely_N$alpha, 
                    rely_A$alpha, 
                    rely_C$alpha, 
                    rely_O$alpha),
           Omega = c(rely_E$omega.tot, 
                     rely_N$omega.tot, 
                     rely_A$omega.tot, 
                     rely_C$omega.tot, 
                     rely_O$omega.tot)) %>% pander()
```

# teste t

## Média comparada com uma referência.
Eu sei que a média de neuroticismo mundial é 3,09 e quero saber se a média brasileira é significativamente diferente ou não da média mundial
```{r}
names(big_five)

big_five_BR <- big_five %>% 
  filter(país == "BR")
  
glimpse(big_five_BR)  

t.test(big_five_BR$neur, mu = 3.09)
```

Encontrou-se uma diferença estatisticamente significativa entre as médias (t = 4,0006, df = 174, p-value = 9,331e-05), em que os participantes brasileiros apresentaram um valor mais elevado (neur_BR = 3,335) do que o de participantes de outros países (neur_outros = 3,09) no traço de neuroticismo.

## Representação em tabela
```{r}
big_five %>% 
  drop_na(neur, país) %>% 
  mutate(país_dic = ifelse(país == "BR", "BR", "OU")) %>% 
  group_by(país_dic) %>% 
  summarise(média = mean(neur),
            "Desvio padrão" = sd(neur)) %>% pander()
```

## Representação gráfica
```{r}
big_five %>% mutate(País = ifelse(país == "BR", "BR", "OU")) %>% drop_na %>% 
  ggplot(aes(x = País, y = neur)) +
  geom_boxplot()
  
```


## T-teste para amostras independentes
```{r}
# 1º passo: verificar a homogeneidade de variâncias entre as categorias da variável independente.
# Teste de Levene - pacote car
install.packages("car")

big_five_BR %>% select(gênero) %>% distinct()

car::leveneTest(extr~gênero,big_five_BR,center=mean)
car::leveneTest(neur~gênero,big_five_BR,center=mean)
car::leveneTest(amab~gênero,big_five_BR,center=mean)
car::leveneTest(cons~gênero,big_five_BR,center=mean)
car::leveneTest(aber~gênero,big_five_BR,center=mean)

big_five_BR_nona <- big_five_BR %>% filter(gênero == "Feminino" | gênero == "Masculino")

t_extr <- t.test(extr~gênero, big_five_BR_nona,var.equal = TRUE)
t_neur <- t.test(neur~gênero, big_five_BR_nona,var.equal = TRUE)
t_amab <- t.test(amab~gênero, big_five_BR_nona,var.equal = TRUE)
t_cons <- t.test(cons~gênero, big_five_BR_nona,var.equal = TRUE)
t_aber <- t.test(aber~gênero, big_five_BR_nona,var.equal = TRUE)
```

# T-teste para amostras pareadas
```{r}
df <- data.frame(id = paste("s",1:30,sep=""),
                 pre_teste = rnorm(30, mean = 5.14, sd = 1.23),
                 pos_teste = rnorm(30, mean = 6.78, sd = 0.93))

t_par <- t.test(df$pre_teste,df$pos_teste, paired = TRUE)

mean(df$pre_teste)
mean(df$pos_teste)

```

# ANOVA de uma via
```{r}
names(rwas)
distinct(rwas, education)

readRDS("rwas.rds")

anova_auth <- aov( auth ~ education, rwas)
summary(anova_auth)

one_way_anova_tab <- 
tableby(education ~ auth,
        digits = 1,
        test = FALSE,
        rwas)
summary(one_way_anova_tab)
```

# ANOVA por medidas repetidas
```{r}
install.packages("ez")
library(ez)

df_aov_mr <- data.frame(id = paste("s",1:100,sep = ""),
                        v1 = round(rnorm(n=100, mean = -1.2, sd = 0.54),digits = 3),
                        v2 = round(rnorm(n=100, mean = -0.7, sd = 0.52),digits = 3),
                        v3 = round(rnorm(n=100, mean = -0.1, sd = 0.57),digits = 3),
                        v4 = round(rnorm(n=100, mean =  0.4, sd = 0.55),digits = 3),
                        v5 = round(rnorm(n=100, mean =  0.5, sd = 0.54),digits = 3))

df_aov1 <- df_aov_mr %>% pivot_longer(cols = 2:6,            # varáveis a serem unidas 
                                      names_to = "tempo",    # nome dessa nova variável
                                      values_to = "valores") # valores da nova variável

mod_df_aov1 <- ezANOVA(data = df_aov1,
                       dv = valores,
                       wid = id,
                       within = tempo,
                       detailed = TRUE,
                       type = 3)
```

